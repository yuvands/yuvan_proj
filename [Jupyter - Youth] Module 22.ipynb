{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and preparing data\n",
    "## 1. Collecting data from the internet\n",
    "### 1.1 Web scraping\n",
    "The internet contains a wealth of data for your Natural Language Processing (NLP) projects. However, the data are usually not ready to be used for NLP projects, as they are not packaged in word document or spreadsheet that can be easily downloadable and further processed. Therefore, we have to learn to collect and process our own data. \n",
    "\n",
    "In this exercise, we will learn how to collect raw data from websites using a process known as 'web scraping'. We will build a web scraper that will collect information websites of your choice!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Importing required libraries/ packages\n",
    "First, we import the packages that are required: \n",
    "- The `requests` package allows our python script to communicate with websites and to 'request' information from those sites. \n",
    "- The beautiful soup package, also known as `bs4` takes the raw information from the websites and provides helpful functions to extract information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully imported requests version 2.24.0\n",
      "You have successfully imported beautifulsoup version 4.9.1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "print (\"You have successfully imported requests version \"+requests.__version__)\n",
    "print (\"You have successfully imported beautifulsoup version \"+bs4.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Your packages are imported! Now, we are ready to start scraping some websites!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Finding out information about a website\n",
    "\n",
    "Now, visit http://www.sl2square.org/case-studies/ to have a look at Sustainable Living Lab's case studies. \n",
    "\n",
    "Once the page is shown, press **ctrl-shift-i (or F12) to look at the HTML of the site.** \n",
    "*Note: HTML stands for Hypertext Markup Language and it is a way for your computer browser (like chrome, internet explorer, safari) to understand how information should be displayed. \n",
    "\n",
    "When you visit a website, your computer uses HTML to send a request (called the 'GET' request) to the internet and waits for a response. The response eventually comes with a response code, and possibly the information requested which is then displayed on your computer browser.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Visit [http://www.sl2square.org/case-studies/](http://www.sl2square.org/case-studies/) and press ctrl-shift-i (or F12) now! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay close attention to anything you see within < angular brackets >. These tags tell the browser where and how to display the information contained within. Tags are used to identify sections of the web page, and we will be using these tags later to locate pieces of information we are interested in. \n",
    "\n",
    "### Task: Can you identify the various tags by looking at the HTML code?\n",
    "  \n",
    "Use the `requests.get function()` to call on the website. The `request.get()` function sends a 'GET' request to the website address and attempts to retrieve a response. The `request.get()` function should return 'Response [200]'. This means that we have successfully received information from the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'http://www.sl2square.org/case-studies/'\n",
    "r = requests.get(base_url)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Try changing the base_url from 'http://www.sl2square.org/case-studies/' to 'http://www.sl2square.org/wrong-address/'. \n",
    "What did the response code become? \n",
    "\n",
    "These codes can help in troubleshooting and solving connection problems, but now, we just need to know that a response of 200 means that your GET request was successful!\n",
    "\n",
    "Connection establised! We have access!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Using beautifulsoup to get data from websites\n",
    "\n",
    "Now we can use `bs4` to read the outputs from our request. \n",
    "\n",
    "Beautiful Soup is a Python library for getting data out of HTML, XML, and other markup languages.\n",
    "*Note: This section of the Jupyter Notebook is written in 'markup' format.\n",
    "\n",
    "Much like our browser, bs4 is able to understand HTML and to read the < tags >;\n",
    "\n",
    "See the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#`r.text` contains the raw HTML returned when we made our GET request earlier. \n",
    "#`'html5lib'` tells BeautifulSoup that it is reading HTML information. \n",
    "soup = bs4.BeautifulSoup(r.text,'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we are taking the raw response from the GET request, and asking BeautifulSoup to read and understand the response, and store all of this as your variable `soup`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Using beautifulsoup to search for specific tags\n",
    "\n",
    "This is where it gets interesting. BeautifulSoup can be used to search for specific tags. \n",
    "\n",
    "Let's use beautifulsoup to look for all the links within the SL2 website!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sl2square.org/portfolio-item/startathon/\n",
      "https://www.sl2square.org/portfolio-item/improving-public-housing/\n",
      "https://www.sl2square.org/portfolio-item/future-of-work/\n",
      "https://www.sl2square.org/portfolio-item/iot-shipment-tracker/\n",
      "https://www.sl2square.org/portfolio-item/solar-dryer/\n",
      "https://www.sl2square.org/portfolio-item/a-star-iot-hack/\n",
      "https://www.sl2square.org/portfolio-item/new-markets-inclusive-design/\n"
     ]
    }
   ],
   "source": [
    "project_list = []\n",
    "for url in soup.findAll(\"a\", {\"class\": \"entry-thumb\"}):\n",
    "    project_list.append(url.get('href'))\n",
    "    print(url.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, we find all `<a>` tags with the class of 'entry-thumb'. We then loop through each tag found and extract the links by looking for 'href' tags.  \n",
    "\n",
    "*Note: The `<a>` tag defines a [hyperlink](https://www.w3schools.com/tags/tag_a.asp), which links to information on different pages. To differentiate `<a>` tags appearing in different locations, they can be 'named' with a `class=tag-name`. In this exercise, the links we are searching for are located within the `<a>` tag, and is named 'entry-thumb'. We can thus search for all elements in the HTML with this tag and class, and to collect all links found, which is marked by the `href` keyword. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see if we have managed to collect all the links in the `project_list`. Show the variable project_list and check if the information has been added into this variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sl2square.org/portfolio-item/startathon/',\n",
       " 'https://www.sl2square.org/portfolio-item/improving-public-housing/',\n",
       " 'https://www.sl2square.org/portfolio-item/future-of-work/',\n",
       " 'https://www.sl2square.org/portfolio-item/iot-shipment-tracker/',\n",
       " 'https://www.sl2square.org/portfolio-item/solar-dryer/',\n",
       " 'https://www.sl2square.org/portfolio-item/a-star-iot-hack/',\n",
       " 'https://www.sl2square.org/portfolio-item/new-markets-inclusive-design/']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will extract the description of each project by (1) visiting each site in the list and (2) looking for the class with the content we are interested in. Let us see how it is done for one link, and you will write code to do it for all links later. \n",
    "  \n",
    "Using the same method as before, visit the first link and press F12 to view page source. Select the element containing the main text and you will be able to see the element source on the panel. Note that the text is surrounded by < P > tags, and also note the class of the < div > tag above the text. We will be using this information to extract the text soon. Come, do that now!\n",
    "  \n",
    "Notice that on this page, the information is stored within a < div > tag, and it has been given the `class` of 'entry-content content'. We want to find all paragraphs within these divs, so we look for < P >  tags, and we call `.get_text()` to retrieve the actual words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-3-b97bbf586d0f>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-b97bbf586d0f>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(project_list[1])\n",
    "soup = bs4.BeautifulSoup(r.text,'html5lib')\n",
    "deet = soup.find('div',{'class':'entry-content content'}) # Search for div tags of class 'entry-content content'\n",
    "all_para = deet.findAll('p') # Within these tags, find all p tags\n",
    "for para in all_para:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can see how it is possible to extract text from a web page, it is your turn!  \n",
    "### Task 1: Write a function that can loop through the urls in `project_list` and extract the description of each project as a string. Combine each string into a list called `project_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge\n",
      "Nanyang Technopreneurship Centre seek to encourage more students and young adults to embark in technopreneurship through IdeasInc, an accelerator programmes, and #startathon, a technopreneurship-focused hackathon. However, the barrier for those with no technical background seemed to be very high.\n",
      "Insight\n",
      "Many students and young adults with no technical background feel that they would not be able to compete or contribute in technical hackathon. Therefore, they would be unlikely to join such hackathon.\n",
      "Solution\n",
      "We designed a hackathon to cater to those with non technical background. This is done by providing live masterclasses to teach basic making skills, around-the-clock technical consultants to help the participants, and even a mini maker-space to push those with technical skills to try making using ‘traditional’ tools such as the band saw and hammers!\n",
      "Result\n",
      "We have been organising #startathon for 4 years now, and have seen constant rise in first-time hackathon participation numbers, with some of #startathon alumni continuing to become an avid maker, even technopreneurs.\n",
      "Challenge: The Housing Development Board (HDB) in Singapore has been running surveys among communities to crowd-source for innovative tech-powered solutions to enhance the safety and vibrancy of the HDB estate. However, the development and implementation of these ideas has shown itself to be a challenge, as the hackathon format lends itself to weekend innovators who typically have no intention of carrying forward their project. \n",
      "Insight: HDB would benefit from having an open innovation process that not only brings together a focused group of capable and committed innovators, but also enables them with appropriate prototyping resources to create high resolution Minimum Viable Products (MVPs) that could be tested rapidly with the public. Current hackathon formats and standard challenges usually do not have processes in place to incentivise participants or equip them with the resources to transform their ideas into a working prototypes, leading to a lack of tangible output at the end of the process. Additionally, ideas are also not typically put through testing at this stage leading to a lots of product-market fit issues. This is a gap we decided to fill.\n",
      "\n",
      "In the first stage, we held an open call for ideas. In the the second stage, the teams with the most promising ideas were given a chance to create and test their ideas within a time frame of up to 3 months. The programme was designed with a Design Thinking approach to help participants create user centered solutions. To enable participants to empathize with HDB residents, a community mapping exercise was conducted in several public housing estates. Workshops were also conducted for participants in the areas of lean project management, rapid prototyping and performing user testing.\n",
      "Participants were also supported in the 3 month development phase by mentors who had expertise in Internet of Things (IoT), Gamification methods and Geo-Spatial Mapping. Testing sessions were held for the participants to get direct user feedback, before they entered their iteration process to refine their product. We also brought in technology partners such as Microsoft and IBM to up-skill participants on their respective cloud platforms (Azure & Bluemix).\n",
      "Finally, a pitching session was held for participants could pitch their high-resolution and tested solutions to HDB’s external partners who had commercial interest and/or access to follow-on funding. The entire process ensured that the participants were given all the support they needed to fully realize their ideas in a tangible form.\n",
      "Result: The entire open innovation process produced stunning results. Participants proved their potential and showed success in using their technology skills to add value to the community. The resulting projects included an IoT-powered community gardening platform, Visualization of to-be-built HDB flats using Augmented Reality, and a mobile application to encourage the vibrancy of the neighbourhood. Out of all participating teams, 6 entered the final stage and from them, 4 groups were ‘adopted’ by external partners of HDB and given follow-on funding for their projects.\n",
      "The consequences of the driving forces that are shaping the Future of Work will be far-reaching and fundamentally transform the nature of work, the definition of workers and the concept of workplaces.\n",
      "The two main driving forces of technological change (integration of Artificial Intelligence & Automation into work tasks) and the rise of long term contingent workers will undoubtedly create new opportunities for some, but for many, they represent the fear of a unsustainable and uncertain future. \n",
      "This topic is of deep interest to us and our partners as it strikes at the heart of multiple Sustainable Development Goals (SDGs) centered around poverty alleviation, economic inclusion, and the development of sustainable communities. Listed below is a snapshot of the work we have done with private companies, public sector organizations and trade associations.\n",
      "Challenge: To transform bank employees into a future-ready workforce.\n",
      "Solution : A 1 x Day Futures Thinking workshop was conducted for the combined HR department. Participants were equipped to embrace a futures mindset and introduced to methods to anticipate, prepare and take advantage of disruption in the Future of Work. Among the methods covered were STEEP analysis to scan for emerging trends and Causal Layered Analysis to deepen their understanding of specific driving forces. \n",
      "Success: Participants of the workshop left with a toolkit of Futures Thinking methods that enabled them to test the relevance of their current HR strategies and develop new HR strategies that would thrive in the Future of Work.\n",
      "Challenge: Junior Chamber International (JCI) is a nonprofit organization of young active citizens aged 18 to 40 who meet in regional, national and international conferences to work towards their current mission which is to provide development opportunities that empower young people to create positive change. With Indonesia having one of the largest youth populations in the world, JCI’s Indonesian members wanted their members to be ready for the Future of Work.\n",
      "\n",
      "Solution : A 3 hour introductory Futures Thinking workshop was conducted in a conference format for JCI members. Given the large number of participants, collaborative hands-on exercises were conducted to crowd-source important STEEPV (social, technological, economic, environmental, political, values) driving forces. The session rounded off with a Futures Triangle exercise to explore the space of plausible futures. \n",
      "Success: Participants of the workshop left with exposure to a completely new way of anticipating change. The projects within JCI (Medan) began to be more long-term in nature and revolved less within the 1 year time frame of each leadership cycle.\n",
      "Challenge\n",
      "RAITONG Organics Farm in Thailand was suffering from possible theft of their goods. Several boxes of organic rice was found missing during transport delivery from the farm in the North-Eastern Thailand to the shipping port at Bangkok. \n",
      "Insight\n",
      "It was required to find out exactly where and when the rice were lost to get transparency and accountability from the responsible party.    \n",
      "Solution\n",
      "An Internet of Things (IoT) system to track the location of the truck and the rice boxes within it. A simple-to-deploy tripwire system was installed on strategic location, which will trigger a message to be sent when truck stopped moving for a set amount of time, and when the tripwire was set off. The full solution can be viewed at Hackster.io\n",
      "Result\n",
      "The prototype gives occasional false-positive results. Development of the second version is ongoing. Meanwhile,  RAITONG Organics Farm is able to track the truck and hold their transport providers accountable. \n",
      "Challenge\n",
      "As part of the UN’s Mondialogo Challenge, our team analysed the problem of farmers suicide in India. \n",
      "Insight\n",
      "Oversupply  was causing a drop in price and therefore income of the local farmers. They were unable to sell their excess crops, and had to leave them to waste. \n",
      "Solution\n",
      "We developed a solar dryer suitable for the local climate, and supported the local economy by establishing connection between the local farmers and the potential buyers for dried goods.\n",
      "Result\n",
      "After establishing the initial market for the farmers, we worked with a local company to craft a sustainable business model for the solar dryer.\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge\n",
      "A*STAR has just developed several IoT hardwares and APIs, and it was looking for potential usage of these technologies.\n",
      "Insight\n",
      "A*STAR would benefit from having a small group of highly skilled innovators working on the organisation’s areas of interests, as the learning curve to pick up new technology is rather high.\n",
      "Solution\n",
      "We designed a two-pronged open innovation programme; one was for a selected group of innovators to ideate and prototype products using the new technology. Another one was to involve our in-house experts to produce and deploy prototypes throughout Singapore\n",
      "Result\n",
      "Hackathon participants managed to produce more than 5 possible applications for A*STAR, and our in-house team successfully deployed more than 100 IoT sensors throughout Singapore to support A*STAR product development process.\n",
      "Challenge: Nippon Closures Co. Ltd (NCC), a leading Japanese packaging company that manufactures and sells metal and plastic closures, wanted to innovate and develop new business propositions as part of their market expansion activities in South-East Asia. There was a desire for their upcoming innovations in closures to address societal needs, such as closures that are easier for senior citizens to open – a response to the aging population in Japan and other developed societies.\n",
      "Insight: A design thinking process would be very useful to create products if we were solely focusing on the needs of the elderly. However, NCC’s consumer grade products had the potential to be used by all types of users, including those with disabilities. We felt that adopting universal design principles would increase the scale of users that NCC could impact – in line with SDG 10 (Reduced Inequalities). However Universal Design is not a popularly known concept and further more, many designers would not have had significant interactions with persons of disabilities.\n",
      "Additionally, it was realized that designers, while talented, were unlikely to be aware about NCC’s machine specifications. After all, any new design must be producible by NCC in order to see the light of day. Our solution, had to take all of these requirements into account.\n",
      "\n",
      "We decided to hold an open challenge in Singapore to design for the general public with the incorporation of inclusive design principles to accommodate both the elderly and persons with disabilities. In the first part of the process, over 30 designs were submitted online by interested design teams. These were whittled down to 11 designs through several rounds of evaluation on technical feasibility, financial viability and customer desirability aspects.\n",
      "The participants, mainly designers, enjoyed the intense design and technical mentorship provided by the product development engineers of NCC to redesign the bottle caps over a two-month period. We brought in expert speakers to conduct workshops on Universal Design and Design for Manufacturing for participants. The mentoring process as well as the workshops not only benefited participants but also members of NCC’s own product design team who were exposed to open innovation methods for the first time.\n",
      "For practicality of design, we also had participants work closely with target users to provide timely feedback. We also partnered veteran designers, potential customers of NCC as well as members of the disability community to attend the product development process as mentors and judges. The finale and exhibition was held at the National Design Centre with several dignitaries and clients of NCC in attendance.\n",
      "Result: 11 prototypes were created and 5 were selected for further testing, refinement and Intellectual Property (IP) registration. NCC successfully aligned themselves to the missions of several NGOS and design companies interested in universal design, which puts them in good stead to accelerate their business development in Southeast Asia. The parent company of NCC has also been inspired by the open innovation process to setup a product design innovation lab in Singapore in early 2018. The redesigned packaging is expected to be launched at the 2020 Summer Paralympics held in Japan.\n"
     ]
    }
   ],
   "source": [
    "project_text=[]\n",
    "for project in project_list:\n",
    "    r = requests.get(project)\n",
    "    soup = bs4.BeautifulSoup(r.text,'html5lib')\n",
    "    deet = soup.find('div',{'class':'entry-content content'})\n",
    "    all_para = deet.findAll('p')\n",
    "    individual_proj=''\n",
    "    for para in all_para:\n",
    "        print(para.get_text())\n",
    "        individual_proj+=para.get_text()\n",
    "    project_text.append(individual_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Storing data\n",
    "Congratulations! You have built your very own scraper and used it to collect data from several webpages. You now need a way to store this data so you do not have to scrape the websites every time you want to access the data. Storing the information collected also preserves the data in the event that the website is taken down or changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Write code to save your data as a text file. \n",
    "\n",
    "If you need help, try https://stackoverflow.com/questions/899103/writing-a-list-to-a-file-with-python or searching on your own using any search engine of your choice.\n",
    "\n",
    "*Note: You will have to create a file object with open(filename). Then use the .write() attribute to write text into the file. See the documentation for writing to file at https://www.guru99.com/reading-and-writing-files-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sl2.txt', 'w') as file_handler:\n",
    "    for item in project_text:\n",
    "        file_handler.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now check your file on your directory folder and see if you have the required information captured there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Other data sources\n",
    "Congratulations! you are now equipped to download text from any website. Do take note that different websites have different html structures, so you will have to modify your code for each site you scrape. \n",
    "\n",
    "How then can we get alot of data without caring too much on the differences between the websites?\n",
    "\n",
    "#### How else can we get lots of data?  \n",
    "  \n",
    "Fortunately, there are great amount of data that has already been compiled by other people. These files may be saved in a document format such as .csv or .txt which can be easily downloaded with a simple script. \n",
    "\n",
    "There are sites dedicated to harvesting interesting datasets which can then be easily downloaded by anyone and used for AI.\n",
    "Check out where you can get free, processed data for your projects: \n",
    "- https://www.figure-eight.com/data-for-everyone/.  \n",
    "- https://github.com/niderhoff/nlp-datasets\n",
    "\n",
    "Take a look at some of the datasets and think of things you would like to work on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading tweet data \n",
    "For now, we will be looking at a dataset that captures social media responses in the from of tweets.\n",
    "\n",
    "This dataset consists of a series of tweets, along with an indication of whether the tweet is relevant to a disaster, or not relevant to a disaster. \n",
    "\n",
    "How do you think this dataset can be used to train an AI system? How do you think this dataset can be helpful?  \n",
    "  \n",
    "Being able to classify tweets will allow us to filter out noise and gain information from useful tweets in real time during an actual disaster. These tweets may contain helpful information on places to avoid, how to help, or how to ask for help.  \n",
    "  \n",
    "The code below downloads and saves a .csv file located at `url` using the library `urllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/glrn/nlp-disaster-analysis/master/dataset/socialmedia-disaster-tweets-DFE.csv'\n",
    "csv = urllib.request.urlopen(url).read()\n",
    "with open('./disasters_social_media.csv', 'wb') as fx:\n",
    "    fx.write(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've now downloaded our data! Check out the csv file we have just created! \n",
    "\n",
    "Notice how the dataset is structured. \n",
    "What headers do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Processing text\n",
    "In its raw form, tweets are just strings of text. While humans can easily read and understand strings of text, computers have a much harder time doing so. Therefore, we will have to perform preprocessing on these strings to break them up into a form that our computers can recognize and work on. \n",
    "\n",
    "Preprocessing the text will also allow us to analyze and visualize the text, and to look for trends and features that your computer will then be able to use to classify your text.  \n",
    "  \n",
    "Let us practice using the dataset on tweet disasters.  \n",
    "\n",
    "First, we import the packages we need:\n",
    "- The pandas package helps us to load files into our program in a structure called a dataframe\n",
    "- The nltk package is the star of the show that allows us to process and visualize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully imported pandas version 1.0.5\n",
      "You have successfully imported nltk version 3.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "print (\"You have successfully imported pandas version \"+pd.__version__)\n",
    "print (\"You have successfully imported nltk version \"+nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have successfully loaded your csv file\n"
     ]
    }
   ],
   "source": [
    "# load the csv file that contains the tweets on natural disasters into a pandas dataframe\n",
    "df_raw = pd.read_csv('./disasters_social_media.csv', encoding='latin-1')\n",
    "\n",
    "print (\"You have successfully loaded your csv file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at the data!  \n",
    "using `.head(5)` on a pandas dataframe lets us look at the first 5 lines of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one</th>\n",
       "      <th>choose_one:confidence</th>\n",
       "      <th>choose_one_gold</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>778243823</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778243824</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>778243825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>778243826</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>0.9603</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778243827</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Relevant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  778243823     True      golden                 156               NaN   \n",
       "1  778243824     True      golden                 152               NaN   \n",
       "2  778243825     True      golden                 137               NaN   \n",
       "3  778243826     True      golden                 136               NaN   \n",
       "4  778243827     True      golden                 138               NaN   \n",
       "\n",
       "  choose_one  choose_one:confidence choose_one_gold keyword location  \\\n",
       "0   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "1   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "2   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "3   Relevant                 0.9603        Relevant     NaN      NaN   \n",
       "4   Relevant                 1.0000        Relevant     NaN      NaN   \n",
       "\n",
       "                                                text  tweetid  userid  \n",
       "0                 Just happened a terrible car crash      1.0     NaN  \n",
       "1  Our Deeds are the Reason of this #earthquake M...     13.0     NaN  \n",
       "2  Heard about #earthquake is different cities, s...     14.0     NaN  \n",
       "3  there is a forest fire at spot pond, geese are...     15.0     NaN  \n",
       "4             Forest fire near La Ronge Sask. Canada     16.0     NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you inspect the first 10 lines of the dataframe? Try it on the code block above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas dataframe looks like a spreadsheet, with each column containing a category of data, and each row representing the information from a single tweet.  \n",
    "\n",
    "There are many columns, and we will not worry about those for now. Let us focus our attention on the column where the tweets are stored. \n",
    "\n",
    "See the dataframe above! Can you tell which column we should inspect to see the tweets? putting the column name in string format inside square brackets lets you select just that column. i.e. `df_raw['column_name']`\n",
    "\n",
    "### Task: Try out different columns and see the data included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    778243823\n",
       "1    778243824\n",
       "2    778243825\n",
       "3    778243826\n",
       "4    778243827\n",
       "Name: _unit_id, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['_unit_id'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at a few random samples of tweets. use .sample() to view the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entertain this thought for a moment:\\n\\ndiarrhea hurricane']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_raw['text'].sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you view 5 random samples? Remember what you did earlier with `.head(5)`? Try it above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each row is data collected from a single tweet, how can we find the total number of tweets in the dataset?  \n",
    "hint: use `len(dataframe)` to count the number of rows in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10876"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "len(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing on the text data\n",
    "\n",
    "Excellent! Let us now move into just the text. Since we do not need the rest of the columns, let us create a new dataframe that consists of just the text. It is a good practice to put 'df' in front of the names of dataframes to help you remember that you are working with an object of type:dataframe. using `.copy()` makes sure that you do not modify the original, raw, dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = df_raw['text'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataframe you have just created contains what you expect. Print out the first few lines of your new dataframe and check that they only contain the tweets in the 'text' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   Just happened a terrible car crash\n",
       "1    Our Deeds are the Reason of this #earthquake M...\n",
       "2    Heard about #earthquake is different cities, s...\n",
       "3    there is a forest fire at spot pond, geese are...\n",
       "4               Forest fire near La Ronge Sask. Canada\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 NLP data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The first step in the NLP task is to tokenize your tweets. Do you remember what tokenization does? let us see an example! We will use a single tweet to demonstrate what each process does. The tokenization is done by calling nltk.tokenize.word_token(your data here). See if you can tell what the function did by printing the text before and after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-56db9d599aa6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Before tokenization:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_text' is not defined"
     ]
    }
   ],
   "source": [
    "sample_tweet = df_text.iloc[100]\n",
    "print('Before tokenization:', sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0650db0aa044>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'After tokenization:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenized_tweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_tweet = nltk.tokenize.word_tokenize(sample_tweet)\n",
    "print('After tokenization:', tokenized_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell what the tokenization step did? We can easily tell what individual words are when given a sentence but computers can't. We need to break the sentence up into words for the computer.\n",
    "  \n",
    "Another benefit of breaking up the sentence is that we can now measure the number of unique words in the corpus (our collection of texts). This is known as the vocabulary of the corpus. \n",
    "\n",
    "To do this we use a technique called [list comprehension](https://www.pythonforbeginners.com/basics/list-comprehensions-in-python). The objective is to put all tokens in all tweets into a single list, then finding the number of unique values in that list of tokens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cee30cf46d37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Creates a list of tweets, with each tweet being a list of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_raw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Flattens the lists into a single list, then counts the number of unique tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_text' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_raw = [nltk.tokenize.word_tokenize(x) for x in list(df_text)] # Creates a list of tweets, with each tweet being a list of tokens\n",
    "len(set([y for x in tokenized_raw for y in x])) # Flattens the lists into a single list, then counts the number of unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that there are 35335 unique tokens in the tweet dataset. That seems like a lot of words! One of the goals of text normalization is to reduce the number of words in the vocabulary by eliminating redundant words without sacrificing the meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemming\n",
    "\n",
    "Now, let us look at what stemming and lemming does to the same sample tweet. We use the Porter Stemmer provided by `nltk.stem.PorterStemmer()`, as well as the Word Net lemmatizer by calling `nltk.stem.WordNetLemmatizer()`. Look at the printed outputs and compare them to the tokenized text above to spot the difference of each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-0b32b41a17a7>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-0b32b41a17a7>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    lemmed = [wnl.lemmatize(word) for word in  # your code here\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.stem.PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stemmed = [porter.stem(word) for word in  # your code here\n",
    "print(stemmed)\n",
    "lemmed = [wnl.lemmatize(word) for word in  # your code here\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmed = [wnl.lemmatize(word) for word in tokenized_tweet]\n",
    "print(lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell what they do? [The difference](https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/) is subtle. \n",
    "\n",
    "A stemmer works by shortening words so that words with the same meaning but different forms will end up with the same token. For example, the token 'accident' was stemmed to 'accid'. Other similar words such as 'accidentally', 'accidental', and 'accidents' will all be stemmed to 'accid'. This helps to group words with similar meanings together.  \n",
    "  \n",
    "A lemmatizer is slightly different. It searches for words with the same meaning and replaces them with the root word. So words with the same meaning but quite different spellings will be understood by the computer to be a single word. The disadvantage of a lemmer is that it can take a long time to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop-words\n",
    "Next, we normalize the text by removing stop-words. Do you remember what stopwords are? nltk provides us with a list of stop words, let's see what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = nltk.corpus.stopwords.words('english')\n",
    "# Add a few more stop words we would like to remove here\n",
    "stop.append('@')\n",
    "stop.append('#')\n",
    "stop.append('http')\n",
    "stop.append(':')\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what each piece of code does, we can build a function that takes in the dataframe, and returns the tokenized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    tokenized_tweet = nltk.tokenize.word_tokenize(# your code here)\n",
    "    stemmed = [porter.stem(word) for word in # your code here]\n",
    "    processed = [w.lower() for w in stemmed if w not in stop]\n",
    "    return processed\n",
    "\n",
    "def tokenizer(df):\n",
    "    tweets = []\n",
    "    for _, tweet in df.iterrows():\n",
    "        tweets.append(process_tweet(tweet['text']))\n",
    "    return tweets\n",
    "\n",
    "tweets = tokenizer(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay close attention to the processed tweets. Do you notice that there are many words that are cut (e.g. terribl instead of terrible)? Do you remember why this is done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with your data and the processing a little bit more. What happens when you use a lemmer instead of a stemmer? What other stopwords can you exclude? What about removing punctuation?\n",
    "  \n",
    "Now, print out the size of your processed vocabulary. How many words does it have now? Is it fewer or more than the original vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([y for x in tweets for y in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these pre-processing steps, we have taken raw tweet data, chopped it up into individual tokens, removed stop words, stemmed, and made it all lower case. This has decreased our vocabulary size by almost 10000 tokens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Let us visualize the data now. We are going to look at the distribution of the number of words in each tweet. This can be done using a histogram. What is the most common number of words in a tweet? In the process of classifying your dataset, your model may look at factors such as the length of the tweet, the number of words etc. Plotting your data out also helps you to ensure that your code is working, and not throwing out ridiculous numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_hist(tweets):\n",
    "    sentence_lengths = [len(tokens) for tokens in tweets]\n",
    "    fig = plt.figure(figsize=(6, 6)) \n",
    "    plt.xlabel('Tweet length')\n",
    "    plt.ylabel('Number of tweets')\n",
    "    plt.hist(sentence_lengths, bins=20)\n",
    "    plt.show()\n",
    "    return sentence_lengths\n",
    "tweet_lengths = plot_hist(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the average tweet has about 12-15 words, and that there are a few with almost 60 words. What exactly is the length of the shortest tweet, the length of the longest tweet, and the average length of all tweets? Print those values out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting to know our data more and more now. Let us dive deeper and see if we can determine if there is a difference in the average length of tweets for relevant and not relevant tweets. Perhaps it can be used as a good feature for future analyis.\n",
    "  \n",
    "To do that, we use the `'choose_one'` column in `df_raw`. If the value in the column is 'Relevant', the tweet is classified as relating to a natural disaster. If the value is 'Not Relevant', the tweet does not relate to a natural disaster.  \n",
    "  \n",
    "Can you print out the average length of tweet for relevant tweets, and the average length of tweet for not relevant tweets?\n",
    "  \n",
    "*Hint: the dataframe `df_result` has been constructed to contain the processed tweets and the relevance. create two dataframes, one containing only 'Relevant' tweets, and one containing only 'Not Relevant' tweets. This can be done by subsetting the dataframe: `df_result[df_result['choose_one']=='Relevant']`. We have given an example of how to do it to create a dataframe of positive examples. Can you create the dataframe of negative examples?  \n",
    "\n",
    "*Hint 2: To calculate the length of each tweet, you can use the `.apply(len)` function. You can also use `.mean()` to get the mean of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_raw['choose_one'].copy().to_frame()\n",
    "df_result['processed_text'] = tweets\n",
    "df_neg=df_result[df_result['choose_one']=='Not Relevant']\n",
    "df_pos=df_result[df_result['choose_one']=='Relevant']\n",
    "print(df_pos['processed_text'].apply(len).mean(), df_neg['processed_text'].apply(len).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think tweet length make a good feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis challenge\n",
    "\n",
    "Using the skills you have learnt, apply the text processing process on the data you have gathered in the beginning of the notebook (from SL2 website). Use the knowledge of data scraping and text processing you've gathered here. \n",
    "\n",
    "Report the initial and final vocabulary size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now learnt:\n",
    "1. How to look for text data\n",
    "2. How to collect data online using scrapers and various modules\n",
    "3. How to save this data onto your computer\n",
    "4. How to process this text data into a form your computer can understand\n",
    "5. How to analyze and visualize the processed data\n",
    "\n",
    "In the next experience module, we will learn how to take what we have learnt and use it to train an NLP model to actually recognize tweets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
