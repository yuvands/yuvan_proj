{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chatbot with Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've discovered how to build a chatbot with cosine similarity. Now, let's explore how we might build one with neural network!\n",
    "\n",
    "We will create our training data, train a neural network with them, then use the trained model to make our chatbot. \n",
    "\n",
    "First, we will install required libraries. Uncomment the few blocks below only if you do not have the libraries installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: scipy in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from scikit-learn) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: pillow in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (7.2.0)\n",
      "Requirement already satisfied: h5py in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.7 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from h5py) (1.18.5)\n",
      "Requirement already satisfied: six in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from h5py) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy scipy\n",
    "# !pip install scikit-learn\n",
    "# !pip install pillow\n",
    "# !pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.0-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Collecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow) (3.7.4.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=46fb833f7f0e3e632ffd987b3f2da16091a61a0555e446026b163e07dc5aa66f\n",
      "  Stored in directory: c:\\users\\yuvan\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19558 sha256=d912c248765cf7d46691006bc8ad8b87b0ad9feb5e873de8883b392589cfcd24\n",
      "  Stored in directory: c:\\users\\yuvan\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: grpcio, flatbuffers, gast, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, wheel, tensorboard-plugin-wit, numpy, protobuf, absl-py, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, tensorboard, tensorflow-estimator, termcolor, astunparse, opt-einsum, wrapt, keras-preprocessing, google-pasta, tensorflow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading tensorflow_gpu-2.4.0-cp38-cp38-win_amd64.whl (370.7 MB)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (3.7.4.2)\n",
      "Requirement already satisfied: absl-py~=0.10 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (0.11.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.19.5)\n",
      "Requirement already satisfied: h5py~=2.10.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (0.36.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (2.4.0)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.32.0)\n",
      "Requirement already satisfied: gast==0.3.3 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (0.3.3)\n",
      "Requirement already satisfied: six~=1.15.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (3.14.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorflow-gpu) (1.6.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (49.2.0.post20200714)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.24.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from tensorboard~=2.4->tensorflow-gpu) (2.24.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.25.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
      "Installing collected packages: tensorflow-gpu\n",
      "Successfully installed tensorflow-gpu-2.4.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: pyyaml in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: h5py in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: six in d:\\yuvan\\coding\\anaconda navigator\\content\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Libraries\n",
    "\n",
    "Firstly, we will install libraries needed for this neural network powered chatbot. \n",
    "Keras is a machine learning library which utilizes tensorflow (another lower level machine learning library) at the backend. This makes it easier for us to deploy deep neural network for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    " \n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Input training data\n",
    "\n",
    "We will first include the following training data for our chatbot:\n",
    "1. X represent the different possible inputs that users might enter\n",
    "2. Y represent the intent of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Hi',\n",
    "     'Hello',\n",
    "     'How are you?',\n",
    "     'I am making',\n",
    "     'making',\n",
    "     'working',\n",
    "     'studying',\n",
    "     'see you later',\n",
    "     'bye',\n",
    "     'goodbye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ['greeting',\n",
    "     'greeting',\n",
    "     'greeting',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'bye',\n",
    "     'bye',\n",
    "     'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are several different sentences that have similar intent. Here, we are only having 3 intents, but you can add as many as you want for your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way our chatbot will work:\n",
    "1. From the input sentence, we will identify the intent using our trained AI model.\n",
    "2. For each intent, we have a prepared response. \n",
    "\n",
    "For example, if we identify that the intent of the input is for a greeting, we might ask the chatbot to reply with a greeting as well, something like 'hi' or 'how are you doing?'\n",
    "\n",
    "We will use machine learning to create a model that can classify input sentence into different intents. \n",
    "We make it as follows:\n",
    "\n",
    "1. We create a training data (X and Y above) which contains a list of sentences and their intents.\n",
    "2. Use the training data to train a classifier. \n",
    "3. Vectorize input sentences and use classifier to determine intent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text processing\n",
    "\n",
    "As usual, we will start with text processing. Do you remember the process?\n",
    "\n",
    "## 3.1 Remove non alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha_numeric_characters(sentence):\n",
    "    new_sentence = ''\n",
    "    for alphabet in sentence:\n",
    "        if alphabet.isalpha() or alphabet == ' ':\n",
    "            new_sentence += alphabet\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    X = [data_point.lower() for data_point in X]\n",
    "    X = [remove_non_alpha_numeric_characters(\n",
    "        sentence) for sentence in X]\n",
    "    X = [data_point.strip() for data_point in X]\n",
    "    X = [re.sub(' +', ' ',\n",
    "                data_point) for data_point in X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_data(X)\n",
    "\n",
    "vocabulary = set()\n",
    "for data_point in X:\n",
    "    for word in data_point.split(' '):\n",
    "        vocabulary.add(word)\n",
    "\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = []\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    sentence = preprocess_data([sentence])[0]\n",
    "    sentence_encoded = [0] * len(vocabulary)\n",
    "    for i in range(len(vocabulary)):\n",
    "        if vocabulary[i] in sentence.split(' '):\n",
    "            sentence_encoded[i] = 1\n",
    "    return sentence_encoded\n",
    "\n",
    "X_encoded = [encode_sentence(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(Y))\n",
    "\n",
    "Y_encoded = []\n",
    "for data_point in Y:\n",
    "    data_point_encoded = [0] * len(classes)\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] == data_point:\n",
    "            data_point_encoded[i] = 1\n",
    "    Y_encoded.append(data_point_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_encoded\n",
    "y_train = Y_encoded\n",
    "X_test = X_encoded\n",
    "y_test = Y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and check the data you are using for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does y_train represent? Do you understand the array shown above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the training data to train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 1.0783\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0769\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0752\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0734\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0717\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0700\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0685\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0670\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0656\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0641\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0626\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0609\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0592\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0573\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0553\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0532\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0511\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0489\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0467\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0446\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0424\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0402\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0380\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0358\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0336\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0314\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0291\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0269\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0246\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0223\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0200\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0177\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0153\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.0130\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0107\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0084\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0060\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0037\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0014\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9990\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9967\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9944\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9920\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9897\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9873\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 986us/step - loss: 0.9850\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9826\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9803\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9779\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9755\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9732\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9708\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9684\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9661\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9637\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9613\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9590\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9566\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9542\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9518\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9494\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9470\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9446\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9422\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9398\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9374\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9350\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9326\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9302\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9278\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9254\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9230\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9205\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9181\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9157\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9132\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9108\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9083\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9059\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9034\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9010\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8985\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8961\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8936\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8911\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8886\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.8861\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8837\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8812\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.8787\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8762\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8737\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8711\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8686\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8661\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.8636\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8610\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8585\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8560\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.8534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x170f31c0070>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='sigmoid',\n",
    "                input_dim=len(X_train[0])))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01,\n",
    "                            momentum=0.9, nesterov=True))\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List down predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [argmax(pred) for pred in model.predict(np.array(X_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model now. We will compare the prediction made by the model and our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 9\n",
      "Total: 10\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == argmax(y_test[i]):\n",
    "        correct += 1\n",
    "\n",
    "print (\"Correct:\", correct)\n",
    "print (\"Total:\", len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chatbot now! We will input a sentence, and then see what class is predicted by the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print (\"Enter a sentence\")\n",
    "    sentence = input()\n",
    "    prediction= model.predict(np.array([encode_sentence(sentence)]))\n",
    "    print (classes[argmax(prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize that you can't stop the chatbot? You'll have to add the exit command later (see the previous notebook to find out how to do it. \n",
    "\n",
    "For now, simply press the stop button (interrupt button) above to stop the chatbot. \n",
    "\n",
    "Try it! press the stop button, and try typing something onto the box. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "We have successfully use neural network to map our input to conversation intent. \n",
    "Your challenge is to link the conversation intent to a particular response that the chatbot will say. \n",
    "For example, if the conversation intent is 'greeting', get your chatbot to say a greeting as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence\n",
      "hi\n",
      "Hi!\n",
      "Enter a sentence\n",
      "busy\n",
      "all the best!\n",
      "Enter a sentence\n",
      "bye\n",
      "all the best!\n",
      "Enter a sentence\n",
      "goodbye\n",
      "Goodbye! See you next time!\n",
      "Enter a sentence\n",
      "ciao\n",
      "all the best!\n",
      "Enter a sentence\n",
      "osdagno\n",
      "all the best!\n",
      "Enter a sentence\n",
      "you are fine man\n",
      "Hi!\n",
      "Enter a sentence\n"
     ]
    }
   ],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job! You've successfully created a simple chatbot with neural network! How might you improve the chatbot?\n",
    "You can improve the chatbot by:\n",
    "- Adding more training data\n",
    "- Adding more intent\n",
    "- Focusing on a particular topic and train the chatbot with many training data in that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource:\n",
    "https://blog.eduonix.com/internet-of-things/simple-nlp-based-chatbot-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
